trControl = trainControl(method = "cv",
number = 20,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds")
dt.model <- train(x = dat.b.training[,c(1:7)],
y = factor(dat.b.training$beat.line),
method = "rpart",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds")
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=1)
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=0)
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=3)
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=4)
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=5)
View(dat)
View(dat.b)
View(dat)
# nieve Bayes
nb.model <- train(x = dat.b.training[,c(1:7)],
y = factor(dat.b.training$beat.line),
method = "nb",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary)) %>% invisible()
library(dplyr)
dt.model <- train(x = dat.b.training[,c(1:7)],
y = factor(dat.b.training$beat.line),
method = "rpart",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=5)
pred <- predict(cart_original, dat.b.testing)
confusionMatrix(pred, dat.b.testing$beat.line)
dt.model <- train(x = dat.b.training[,c(1:7)],
y = factor(dat.b.training$beat.line),
method = "rpart",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=5)
pred <- predict(cart_original, dat.b.testing)
confusionMatrix(table(dat.b.testing$beat.line,pred))
set.seed(1234)
set.seed(1234)
dt.model <- train(x = dat.b.training[,c(1:7)],
y = factor(dat.b.training$beat.line),
method = "rpart",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=5)
pred <- predict(cart_original, dat.b.testing)
confusionMatrix(table(dat.b.testing$beat.line,pred))
set.seed(123)
dt.model <- train(x = dat.b.training[,c(1:7)],
y = factor(dat.b.training$beat.line),
method = "rpart",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=5)
pred <- predict(cart_original, dat.b.testing)
confusionMatrix(table(dat.b.testing$beat.line,pred))
dat.b.training[,c(1:7)]
dat.b <- mutate(dat,beat.line=ifelse(margin > line,'yes','no'), margin=NULL, win.p = wins/(wins+losses))
dat.b
dat.b <- mutate(dat,win.p = wins/(wins+losses), beat.line=ifelse(margin > line,'yes','no'), margin=NULL)
dat.b
dat.b.training[,c(1:8)]
# binary dataset
training.index <- createDataPartition(dat.b$team, p = 0.8, list = FALSE)
dat.b.training <- dat.b[training_index, ]
dat.b.testing <- dat.b[-training_index, ]
typeof(dat.b.training)
m <- lm(beat.line~. , dat.b.training)
set.seed(123)
dt.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "rpart",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=5)
pred <- predict(cart_original, dat.b.testing)
confusionMatrix(table(dat.b.testing$beat.line,pred))
importance <- varImp(dt.model, scale = FALSE)
plot(importance, main = "Variable Importance")
importance <- varImp(nb.model, scale = FALSE)
importance <- varImp(rv.model, scale = FALSE)
plot(importance, main = "Variable Importance")
rv.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "ranger",
importance = "impurity",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rv.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "ranger",
importance = "impurity",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
dat.b <- mutate(dat,
win.p = wins/(wins+losses),
beat.line=ifelse(margin > line,'yes','no'),
margin=NULL)
data.b[is.nan(data.b)] <- 0.5
dat.b <- mutate(dat,
win.p = wins/(wins+losses),
beat.line=ifelse(margin > line,'yes','no'),
margin=NULL)
dat.b[is.nan(dat.b)] <- 0.5
dat.b <- mutate(dat,
win.p = wins/(wins+losses),
beat.line=ifelse(margin > line,'yes','no'),
margin=NULL)
dat.b$win.p[is.nan(dat.b)] <- 0.5
dat.b <- mutate(dat,
win.p = wins/(wins+losses),
beat.line=ifelse(margin > line,'yes','no'),
margin=NULL)
dat.b$win.p[is.nan(dat.b$win.p)] <- 0.5
rv.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "ranger",
importance = "impurity",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
# binary dataset
training.index <- createDataPartition(dat.b$team, p = 0.8, list = FALSE)
dat.b.training <- dat.b[training_index, ]
dat.b.testing <- dat.b[-training_index, ]
rv.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "ranger",
importance = "impurity",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
pred <- predict(rv.model, dat.b.testing, type='prob')
table(pred, dat.b.testing$beat.line)
print('123')
rv.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "ranger",
importance = "impurity",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
pred <- predict(rv.model, dat.b.testing, type='prob')
table(pred, dat.b.testing$beat.line)
pred.result <- predict(rv.model, dat.b.testing)
table(pred.result, dat.b.testing$beat.line)
pred <- predict(cart_original, dat.b.testing)
pred <- predict(dt.model, dat.b.testing)
confusionMatrix(table(dat.b.testing$beat.line,pred))
table(pred.result, dat.b.testing$beat.line)
importance <- varImp(rv.model, scale = FALSE)
plot(importance, main = "Variable Importance")
nb.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "nb",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary)) %>% invisible()
nb.pred.results <- predict(nb.model, dat.b.testing)
table(nb.pred, dat.b.testing$beat.line)
table(nb.pred.results, dat.b.testing$beat.line)
confusionMatrix(table(nb.pred.results, dat.b.testing$beat.line))
importance <- varImp(nb.model, scale = FALSE)
nb.pred <- predict(nb.model, dat.b.testing, type='prob')
nb.pred <- predict(nb.model, dat.b.testing, type='prob') %>% invisible()
nb.pred <- predict(nb.model, dat.b.testing, type='prob')
summary(models.resampling)
library(dplyr)
library(rvest)
library(caret)
library(rpart.plot)
library(caret)
library(e1071)
library(dplyr)
# scrape data
games.2019 <- read_html('https://sportsdatabase.com/nba/query?output=default&sdql=team%2C+site%2C+o%3Ateam%2C+line%2C+streak%2C+margin%2C+wins%2C+losses+%40season%3D2019&submit=++S+D+Q+L+%21++') %>% html_table(fill=TRUE)
dat <- data.frame(games.2019[4])
# feature engineering
dat.b <- mutate(dat,
win.p = wins/(wins+losses),
beat.line=ifelse(margin > line,'yes','no'),
margin=NULL)
# replace nan win percentages with 0.500
dat.b$win.p[is.nan(dat.b$win.p)] <- 0.5
# create binary target variable
training.index <- createDataPartition(dat.b$team, p = 0.8, list = FALSE)
dat.b.training <- dat.b[training_index, ]
dat.b.testing <- dat.b[-training_index, ]
# decision tree model
set.seed(123)
dt.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "rpart",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=5)
pred <- predict(dt.model, dat.b.testing)
confusionMatrix(table(dat.b.testing$beat.line,pred))
importance <- varImp(dt.model, scale = FALSE)
plot(importance, main = "Variable Importance")
# Naive Bayes Model
nb.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "nb",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary)) %>% invisible()
nb.pred <- predict(nb.model, dat.b.testing, type='prob')
nb.pred.results <- predict(nb.model, dat.b.testing)
confusionMatrix(table(nb.pred.results, dat.b.testing$beat.line))
# Random Forest Model
rv.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "ranger",
importance = "impurity",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
pred <- predict(rv.model, dat.b.testing, type='prob')
pred.result <- predict(rv.model, dat.b.testing)
table(pred.result, dat.b.testing$beat.line)
importance <- varImp(rv.model, scale = FALSE)
plot(importance, main = "Variable Importance")
# model comparision
models <- list(decision.tree = dt.model,
naive.bayes = nb.model,
random.forest = rv.model)
models.resampling <- resamples(models)
summary(models.resampling)
bwplot(models.resampling)
dat.b.training <- dat.b[training.index, ]
dat.b.testing <- dat.b[-training.index, ]
set.seed(123)
dt.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "rpart",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
rpart.plot(dt.model$finalModel,
main = "Original CART Model",
box.palette = "Reds",
type=5)
pred <- predict(dt.model, dat.b.testing)
confusionMatrix(table(dat.b.testing$beat.line,pred))
importance <- varImp(dt.model, scale = FALSE)
plot(importance, main = "Variable Importance")
# Naive Bayes Model
nb.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "nb",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary)) %>% invisible()
nb.pred <- predict(nb.model, dat.b.testing, type='prob')
nb.pred.results <- predict(nb.model, dat.b.testing)
confusionMatrix(table(nb.pred.results, dat.b.testing$beat.line))
# Random Forest Model
rv.model <- train(x = dat.b.training[,c(1:8)],
y = factor(dat.b.training$beat.line),
method = "ranger",
importance = "impurity",
tuneLength = 10,
metric = "ROC",
trControl = trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary))
pred <- predict(rv.model, dat.b.testing, type='prob')
pred.result <- predict(rv.model, dat.b.testing)
table(pred.result, dat.b.testing$beat.line)
importance <- varImp(rv.model, scale = FALSE)
plot(importance, main = "Variable Importance")
# model comparision
models <- list(decision.tree = dt.model,
naive.bayes = nb.model,
random.forest = rv.model)
models.resampling <- resamples(models)
summary(models.resampling)
bwplot(models.resampling)
models.roc <- models %>%
map(test_roc, data = dat.b.testing)
library(dplyr)
models.roc <- models %>%
map(test_roc, data = dat.b.testing)
library(purrr)
models.roc <- models %>%
map(test_roc, data = dat.b.testing)
library(ROCR)
install.packages('ROCC')
install.packages('ROCR')
library(ROCR)
performance(pred.result,"tpr","fpr")
performance(pred.result,"sens","spec")
typeof(pred.result)
performance(pred,"sens","spec")
pred <- predict(rv.model, dat.b.testing, type='prob')
performance(pred,"sens","spec")
View(pred)
prediction(pred.result,dat.b.testing$beat.line)
pred.result <- predict(rv.model, dat.b.testing)
prediction(pred.result,dat.b.testing$beat.line)
prediction(data.frame(pred.result),dat.b.testing$beat.line)
pred.binary <- ifelse(pred$yes>0.5,1,0)
label.binary <- ifelse(dat.b.testing$beat.line=='yes',1,0)
prediction(pred.binary,label.binary)
performance(pred,"sens","spec")
prediction <- prediction(pred.binary,label.binary)
performance(prediction,"sens","spec")
plot(performance(prediction,"sens","spec"))
predict(model, data, type = "prob")[, "N"]
predict(dt.model, data, type = "prob")[, "N"]
roc(dat.b$beat.line,
predict(dt.model, data, type = "prob")[, "N"])
install.packages('pROC')
install.packages("pROC")
library(pROC)
roc(dat.b$beat.line,
predict(dt.model, data, type = "prob")[, "N"])
roc(dat.b$beat.line,
predict(dt.model, data, type = "prob")[, "N"])
roc(dat.b$beat.line,
predict(dt.model, data, type = "prob"))
roc(dat.b$beat.line,
predict(dt.model, data, type = "prob"))
roc(dat.b$beat.line,
predict(dt.model, dat.b, type = "prob")[, "N"])
roc(dat.b$beat.line,
predict(dt.model, dat.b, type = "prob"))
roc(dat.b$beat.line,
predict(dt.model, dat.b))
predict(dt.model, dat.b)
contrasts(dat.b$beat.line)
contrasts(factor(dat.b$beat.line))
dat.b$beat.line
factor(dat.b$beat.line)
roc(factor(dat.b$beat.line),
factor(predict(dt.model, dat.b)))
to.bin <- function(x){
ifelse(x=='yes',1,0)
}
roc(to.bin(dat.b$beat.line),
to.bin(predict(dt.model, dat.b)))
sens.ci <- ci.se(roc)
roc <- roc(to.bin(dat.b$beat.line),
to.bin(predict(dt.model, dat.b)))
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")
roc <- roc(to.bin(dat.b$beat.line),
to.bin(predict(nb.model, dat.b)))
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")
roc <- roc(to.bin(dat.b$beat.line),
to.bin(predict(rv.model, dat.b)))
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")
roc <- roc(to.bin(dat.b$beat.line),
to.bin(predict(rv.model, dat.b)),smoothed = TRUE)
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")
# Roc curves
pred
# Roc curves
pred[,'no']
# Roc curves
pred[,'yes']
roc <- roc(pred[,'yes'],
to.bin(predict(rv.model, dat.b)),smoothed = TRUE)
roc <- roc(to.bin(dat.b$beat.line),
to.bin(pred[,'yes']),smoothed = TRUE)
roc.pred <- predict(rv.model, dat.b, type='prob')['yes']
roc <- roc(to.bin(dat.b$beat.line),
to.bin(roc.pred),smoothed = TRUE)
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")
plot(sens.ci, type="shape", col="lightblue")
par()
plot(sens.ci, type="shape", col="lightblue")
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")
plot.new()
plot(sens.ci, type="shape", col="lightblue")
roc.pred <- predict(dt.model, dat.b, type='prob')['yes']
roc <- roc(to.bin(dat.b$beat.line),
to.bin(roc.pred),smoothed = TRUE)
roc <- roc(to.bin(dat.b$beat.line),
roc.pred,smoothed = TRUE)
to.bin(dat.b$beat.line)
roc.pred
roc <- roc(to.bin(dat.b$beat.line),
roc.pred,smoothed = TRUE)
roc.pred
View(roc.pred)
save.image("~/Documents/GitHub/SS2864/Individual Project/models.RData")
library(shiny)
load('models.rdata')
load('models.RData')
setwd("~/Documents/GitHub/SS2864/Individual Project")
load('models.RData')
load('models.RData')
setwd("~/Documents/GitHub/SS2864/Individual Project")
load('models.RData')
load('models.RData')
load('SS2864/models.RData')
setwd("~/Documents/GitHub/SS2864/Individual Project")
load('SS2864/models.RData')
